{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Within your codeup-data-science directory, create a new directory named clustering-exercises. This will be where you do your work for this module. Create a repository on GitHub with the same name, and link your local repository to GitHub.\n",
    "\n",
    "Save your clustering work in your clustering-exercises repo. Then add, commit, and push your changes.\n",
    "\n",
    "For example, if the exercise directs you to create a file named myfile.py, you should have clustering/myfile.py in your repository.\n",
    "\n",
    "If a file extension is specified, you should create that specific file. If there is not file extension specified, you may either create a python script or a jupyter notebook for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Exploring\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Visualizing\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# default pandas decimal number display format\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "import acquire\n",
    "import summarize\n",
    "import prepare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire & Summarize\n",
    "\n",
    "1) Acquire data from mySQL using the python module to connect and query. You will want to end with a single dataframe. Make sure to include: the logerror, all fields related to the properties that are available. You will end up using all the tables in the database.\n",
    "        \n",
    "- Be sure to do the correct join (inner, outer, etc.). We do not want to eliminate properties purely because they may have a null value for airconditioningtypeid.\n",
    "        \n",
    "- Only include properties with a transaction in 2017, and include only the last transaction for each properity (so no duplicate property ID's), along with zestimate error and date of transaction.\n",
    "        \n",
    "- Only include properties that include a latitude and longitude value.\n",
    "\n",
    "2) Summarize your data (summary stats, info, dtypes, shape, distributions, value_counts, etc.)\n",
    "   \n",
    "3) Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an atttribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "\n",
    "1) Remove any properties that are likely to be something other than single unit properties. (e.g. no duplexes, no land/lot, ...). There are multiple ways to estimate that a property is a single unit, and there is not a single \"right\" answer. But for this exercise, do not purely filter by unitcnt as we did previously. Add some new logic that will reduce the number of properties that are falsely removed. You might want to use # bedrooms, square feet, unit type or the like to then identify those with unitcnt not defined.\n",
    "\n",
    "2) Create a function that will drop rows or columns based on the percent of values that are missing: handle_missing_values(df, prop_required_column, prop_required_row).\n",
    "\n",
    "### The input:\n",
    "            \n",
    "- A dataframe\n",
    "            \n",
    "- A number between 0 and 1 that represents the proportion, for each column, of rows with non-missing values required to keep the column. i.e. if prop_required_column = .6, then you are requiring a column to have at least 60% of values not-NA (no more than 40% missing).\n",
    "            \n",
    "- A number between 0 and 1 that represents the proportion, for each row, of columns/variables with non-missing values required to keep the row. For example, if prop_required_row = .75, then you are requiring a row to have at least 75% of variables with a non-missing value (no more that 25% missing).\n",
    "        \n",
    "### The output:\n",
    "            \n",
    "- The dataframe with the columns and rows dropped as indicated. Be sure to drop the columns prior to the rows in your function.\n",
    "       \n",
    "### hint:\n",
    "            \n",
    "- Look up the dropna documentation.\n",
    "            \n",
    "- You will want to compute a threshold from your input values (prop_required) and total number of rows or columns.\n",
    "            \n",
    "- Make use of inplace, i.e. inplace=True/False.\n",
    "\n",
    "3) Decide how to handle the remaining missing values:\n",
    "         - Fill with constant value.\n",
    "         - Impute with mean, median, mode.\n",
    "         - Drop row/column\n",
    "\n",
    "\n",
    "### wrangle_zillow.py\n",
    "\n",
    "Functions of the work above needed to acquire and prepare a new sample of data.\n",
    "Mall Customers\n",
    "\n",
    "# notebook\n",
    "\n",
    "- Acquire data from mall_customers.customers in mysql database.\n",
    "- Summarize data (include distributions and descriptive statistics).\n",
    "- Detect outliers using IQR.\n",
    "- Split data (train, validate, and test split).\n",
    "- Encode categorical columns using a one hot encoder.\n",
    "- Handles missing values.\n",
    "- Scaling\n",
    "\n",
    "# wrangle_mall.py\n",
    "\n",
    "- Acquire data from mall_customers.customers in mysql database.\n",
    "- Split the data into train, validate, and split\n",
    "- One-hot-encoding\n",
    "- Missing values\n",
    "- Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
